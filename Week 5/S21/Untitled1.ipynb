{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPM-ZWzFz03N",
        "outputId": "f04042c3-3c73-4cee-e6e1-d7551e1ec839"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello everyone.',\n",
              " 'Welcome to GeeksforGeeks.',\n",
              " 'You are studying NLP article.']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "#Sentence Tokenization using sent_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "text = \"Hello everyone. Welcome to GeeksforGeeks. You are studying NLP article.\"\n",
        "sent_tokenize(text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Sentence Tokenization using PunktSentenceTokenizer\n",
        "import nltk.data\n",
        "\n",
        "# Loading PunktSentenceTokenizer using English pickle file\n",
        "tokenizer = nltk.data.load('tokenizers/punkt/PY3/english.pickle')\n",
        "tokenizer.tokenize(text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfL6ty3_0Nr_",
        "outputId": "4714333c-70c9-4ea7-e156-fe463eaeac67"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello everyone.',\n",
              " 'Welcome to GeeksforGeeks.',\n",
              " 'You are studying NLP article.']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenize sentence of different language\n",
        "import nltk.data\n",
        "\n",
        "spanish_tokenizer = nltk.data.load('tokenizers/punkt/PY3/spanish.pickle')\n",
        "\n",
        "text = 'Hola amigo. Estoy bien.'\n",
        "spanish_tokenizer.tokenize(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fDjvcnm0cPI",
        "outputId": "7e0fc792-7eca-4578-aebb-c8b0e22bd632"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hola amigo.', 'Estoy bien.']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Word Tokenization using work_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"Hello everyone. Welcome to GeeksforGeeks.\"\n",
        "word_tokenize(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHpaVVxo0i4K",
        "outputId": "6d8c1245-5050-4cd0-8ab8-b3f92087331a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello', 'everyone', '.', 'Welcome', 'to', 'GeeksforGeeks', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Word Tokenization Using TreebankWordTokenizer\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "tokenizer.tokenize(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89acOysX0onY",
        "outputId": "40d11c98-ca58-4eab-83c1-64c4509e9b80"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello', 'everyone.', 'Welcome', 'to', 'GeeksforGeeks', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Word Tokenization using WordPunctTokenizer\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "\n",
        "tokenizer = WordPunctTokenizer()\n",
        "tokenizer.tokenize(\"Let's see how it's working.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RizApN6O0t76",
        "outputId": "42884404-addc-481d-b4a4-9cef9322483b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Let', \"'\", 's', 'see', 'how', 'it', \"'\", 's', 'working', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "ggciNq6Q2MPr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Word Tokenization using Regular Expression\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "text = \"Let's see how it's working.\"\n",
        "tokenizer.tokenize(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GaLXCIGs01-L",
        "outputId": "ec3088eb-0f40-48fe-88db-66f61ddcd04b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Let', 's', 'see', 'how', 'it', 's', 'working']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CyH9j5NU165J"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W6oF_TB018Az"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}