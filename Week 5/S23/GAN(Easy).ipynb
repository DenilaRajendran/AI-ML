{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FyQiRy6Lku8P",
        "outputId": "f610666e-59d2-43b5-986a-8c40889d9b62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x7973546f9c60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x7973546f9c60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2, D Real loss:0.7210008502006531,D Fake Loss:0.7085804343223572,G Loss:[array(0.70858043, dtype=float32), array(0.70858043, dtype=float32), array(0.5, dtype=float32)]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
            "Epoch 4, D Real loss:0.7134621739387512,D Fake Loss:0.7080739736557007,G Loss:[array(0.708074, dtype=float32), array(0.708074, dtype=float32), array(0.5, dtype=float32)]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "Epoch 6, D Real loss:0.7106391787528992,D Fake Loss:0.7073926329612732,G Loss:[array(0.70739263, dtype=float32), array(0.70739263, dtype=float32), array(0.5, dtype=float32)]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
            "Epoch 8, D Real loss:0.7103942632675171,D Fake Loss:0.7081410884857178,G Loss:[array(0.7081411, dtype=float32), array(0.7081411, dtype=float32), array(0.5, dtype=float32)]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
            "Epoch 10, D Real loss:0.7097464203834534,D Fake Loss:0.7080551981925964,G Loss:[array(0.7080552, dtype=float32), array(0.7080552, dtype=float32), array(0.5, dtype=float32)]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
            "Epoch 12, D Real loss:0.7094511985778809,D Fake Loss:0.7079756259918213,G Loss:[array(0.7079756, dtype=float32), array(0.7079756, dtype=float32), array(0.5, dtype=float32)]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
            "Epoch 14, D Real loss:0.7094363570213318,D Fake Loss:0.7084149122238159,G Loss:[array(0.7084149, dtype=float32), array(0.7084149, dtype=float32), array(0.5, dtype=float32)]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
            "Epoch 16, D Real loss:0.7096757292747498,D Fake Loss:0.7083914279937744,G Loss:[array(0.7083914, dtype=float32), array(0.7083914, dtype=float32), array(0.5, dtype=float32)]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
            "Epoch 18, D Real loss:0.7090426683425903,D Fake Loss:0.7081528306007385,G Loss:[array(0.70815283, dtype=float32), array(0.70815283, dtype=float32), array(0.5, dtype=float32)]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
            "Epoch 20, D Real loss:0.7093166708946228,D Fake Loss:0.7084990739822388,G Loss:[array(0.7084991, dtype=float32), array(0.7084991, dtype=float32), array(0.5, dtype=float32)]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
            "Epoch 22, D Real loss:0.7093527913093567,D Fake Loss:0.7085837721824646,G Loss:[array(0.7085838, dtype=float32), array(0.7085838, dtype=float32), array(0.5, dtype=float32)]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
            "Epoch 24, D Real loss:0.709469735622406,D Fake Loss:0.7087361812591553,G Loss:[array(0.7087362, dtype=float32), array(0.7087362, dtype=float32), array(0.5, dtype=float32)]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
            "Epoch 26, D Real loss:0.7096047401428223,D Fake Loss:0.7089358568191528,G Loss:[array(0.70893586, dtype=float32), array(0.70893586, dtype=float32), array(0.5, dtype=float32)]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
            "Epoch 28, D Real loss:0.7101677656173706,D Fake Loss:0.7096450924873352,G Loss:[array(0.7096451, dtype=float32), array(0.7096451, dtype=float32), array(0.5, dtype=float32)]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
            "Epoch 30, D Real loss:0.7103967666625977,D Fake Loss:0.7098768949508667,G Loss:[array(0.7098769, dtype=float32), array(0.7098769, dtype=float32), array(0.5, dtype=float32)]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
            "Epoch 32, D Real loss:0.7105177044868469,D Fake Loss:0.7100680470466614,G Loss:[array(0.71006805, dtype=float32), array(0.71006805, dtype=float32), array(0.5, dtype=float32)]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
            "Epoch 34, D Real loss:0.7104077339172363,D Fake Loss:0.7099200487136841,G Loss:[array(0.70992005, dtype=float32), array(0.70992005, dtype=float32), array(0.5, dtype=float32)]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
            "Epoch 36, D Real loss:0.7104948163032532,D Fake Loss:0.710094153881073,G Loss:[array(0.71009415, dtype=float32), array(0.71009415, dtype=float32), array(0.5, dtype=float32)]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
            "Epoch 38, D Real loss:0.7106919288635254,D Fake Loss:0.7102563381195068,G Loss:[array(0.71025634, dtype=float32), array(0.71025634, dtype=float32), array(0.5, dtype=float32)]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
            "Epoch 40, D Real loss:0.7109127640724182,D Fake Loss:0.7104991674423218,G Loss:[array(0.71049917, dtype=float32), array(0.71049917, dtype=float32), array(0.5, dtype=float32)]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
            "Epoch 42, D Real loss:0.7109277844429016,D Fake Loss:0.7105743885040283,G Loss:[array(0.7105744, dtype=float32), array(0.7105744, dtype=float32), array(0.5, dtype=float32)]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
            "Epoch 44, D Real loss:0.7108421921730042,D Fake Loss:0.7104991674423218,G Loss:[array(0.71049917, dtype=float32), array(0.71049917, dtype=float32), array(0.5, dtype=float32)]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "Epoch 46, D Real loss:0.7110527157783508,D Fake Loss:0.7107295393943787,G Loss:[array(0.71072954, dtype=float32), array(0.71072954, dtype=float32), array(0.5, dtype=float32)]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
            "Epoch 48, D Real loss:0.7108936309814453,D Fake Loss:0.7105762362480164,G Loss:[array(0.71057624, dtype=float32), array(0.71057624, dtype=float32), array(0.5, dtype=float32)]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
            "Epoch 52, D Real loss:0.7112076878547668,D Fake Loss:0.7109256982803345,G Loss:[array(0.7109257, dtype=float32), array(0.7109257, dtype=float32), array(0.5, dtype=float32)]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
            "Epoch 54, D Real loss:0.7112981677055359,D Fake Loss:0.7110276818275452,G Loss:[array(0.7110277, dtype=float32), array(0.7110277, dtype=float32), array(0.5, dtype=float32)]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "Epoch 56, D Real loss:0.7112489342689514,D Fake Loss:0.7109760046005249,G Loss:[array(0.710976, dtype=float32), array(0.710976, dtype=float32), array(0.5, dtype=float32)]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
            "Epoch 58, D Real loss:0.7112435102462769,D Fake Loss:0.7109614610671997,G Loss:[array(0.71096146, dtype=float32), array(0.71096146, dtype=float32), array(0.5, dtype=float32)]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
            "Epoch 60, D Real loss:0.7113344669342041,D Fake Loss:0.7110711336135864,G Loss:[array(0.71107113, dtype=float32), array(0.71107113, dtype=float32), array(0.5, dtype=float32)]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
            "Epoch 62, D Real loss:0.7114184498786926,D Fake Loss:0.7111805081367493,G Loss:[array(0.7111805, dtype=float32), array(0.7111805, dtype=float32), array(0.5, dtype=float32)]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
            "Epoch 64, D Real loss:0.7115126848220825,D Fake Loss:0.7112784385681152,G Loss:[array(0.71127844, dtype=float32), array(0.71127844, dtype=float32), array(0.5, dtype=float32)]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
            "Epoch 66, D Real loss:0.7116757035255432,D Fake Loss:0.7114304304122925,G Loss:[array(0.71143043, dtype=float32), array(0.71143043, dtype=float32), array(0.5, dtype=float32)]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
            "Epoch 68, D Real loss:0.7118090987205505,D Fake Loss:0.7115917205810547,G Loss:[array(0.7115917, dtype=float32), array(0.7115917, dtype=float32), array(0.5, dtype=float32)]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
            "Epoch 70, D Real loss:0.7117673754692078,D Fake Loss:0.7115612626075745,G Loss:[array(0.71156126, dtype=float32), array(0.71156126, dtype=float32), array(0.5, dtype=float32)]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
            "Epoch 72, D Real loss:0.7117292881011963,D Fake Loss:0.7115328311920166,G Loss:[array(0.71153283, dtype=float32), array(0.71153283, dtype=float32), array(0.5, dtype=float32)]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
            "Epoch 74, D Real loss:0.7118032574653625,D Fake Loss:0.71160489320755,G Loss:[array(0.7116049, dtype=float32), array(0.7116049, dtype=float32), array(0.5, dtype=float32)]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
            "Epoch 76, D Real loss:0.7118620872497559,D Fake Loss:0.7116754055023193,G Loss:[array(0.7116754, dtype=float32), array(0.7116754, dtype=float32), array(0.5, dtype=float32)]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
            "Epoch 78, D Real loss:0.7119707465171814,D Fake Loss:0.7117650508880615,G Loss:[array(0.71176505, dtype=float32), array(0.71176505, dtype=float32), array(0.5, dtype=float32)]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "Epoch 80, D Real loss:0.7120116353034973,D Fake Loss:0.7118229269981384,G Loss:[array(0.7118229, dtype=float32), array(0.7118229, dtype=float32), array(0.5, dtype=float32)]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
            "Epoch 82, D Real loss:0.7120949029922485,D Fake Loss:0.7119131088256836,G Loss:[array(0.7119131, dtype=float32), array(0.7119131, dtype=float32), array(0.5, dtype=float32)]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
            "Epoch 84, D Real loss:0.7122215032577515,D Fake Loss:0.7120490074157715,G Loss:[array(0.712049, dtype=float32), array(0.712049, dtype=float32), array(0.5, dtype=float32)]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
            "Epoch 86, D Real loss:0.7122255563735962,D Fake Loss:0.7120528221130371,G Loss:[array(0.7120528, dtype=float32), array(0.7120528, dtype=float32), array(0.5, dtype=float32)]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
            "Epoch 88, D Real loss:0.7122507691383362,D Fake Loss:0.7120910286903381,G Loss:[array(0.712091, dtype=float32), array(0.712091, dtype=float32), array(0.5, dtype=float32)]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
            "Epoch 90, D Real loss:0.7123491168022156,D Fake Loss:0.7121931314468384,G Loss:[array(0.71219313, dtype=float32), array(0.71219313, dtype=float32), array(0.5, dtype=float32)]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "Epoch 92, D Real loss:0.7124468684196472,D Fake Loss:0.7122945189476013,G Loss:[array(0.7122945, dtype=float32), array(0.7122945, dtype=float32), array(0.5, dtype=float32)]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
            "Epoch 94, D Real loss:0.7125828266143799,D Fake Loss:0.7124323844909668,G Loss:[array(0.7124324, dtype=float32), array(0.7124324, dtype=float32), array(0.5, dtype=float32)]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
            "Epoch 96, D Real loss:0.7126076817512512,D Fake Loss:0.7124617695808411,G Loss:[array(0.71246177, dtype=float32), array(0.71246177, dtype=float32), array(0.5, dtype=float32)]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
            "Epoch 98, D Real loss:0.7126121520996094,D Fake Loss:0.7124661803245544,G Loss:[array(0.7124662, dtype=float32), array(0.7124662, dtype=float32), array(0.5, dtype=float32)]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
            "Epoch 100, D Real loss:0.7126203179359436,D Fake Loss:0.712476372718811,G Loss:[array(0.7124764, dtype=float32), array(0.7124764, dtype=float32), array(0.5, dtype=float32)]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "#Generator model\n",
        "def define_generator(latent_dim,output_dim=1):\n",
        "  model=Sequential()\n",
        "  model.add(Dense(10,activation='relu',input_dim=latent_dim))\n",
        "  model.add(Dense(output_dim,activation='linear'))\n",
        "  return model\n",
        "\n",
        "#Discriminator model\n",
        "def define_discriminator(input_dim=1):\n",
        "  model=Sequential()\n",
        "  model.add(Dense(10,activation='relu',input_dim=input_dim))\n",
        "  model.add(Dense(1,activation='sigmoid'))\n",
        "  model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        " #GAN model\n",
        "def define_gan(generator,discriminator):\n",
        "  discriminator.trainable=False\n",
        "  model=Sequential()\n",
        "  model.add(generator)\n",
        "  model.add(discriminator)\n",
        "  model.compile(loss='binary_crossentropy',optimizer='adam')\n",
        "  return model\n",
        "\n",
        "#Generate real samples\n",
        "def generate_real_samples(n):\n",
        "  x=np.random.randn(n)\n",
        "  y=np.ones((n,1))\n",
        "  return x,y\n",
        "\n",
        "#Generate latent poits as input for the generator\n",
        "def generate_latent_points(latent_dim,n):\n",
        "  return np.random.randn(n,latent_dim)\n",
        "\n",
        "#Train the GAN model\n",
        "def train_gan(generator,discriminator,gan_model,latent_dim,n_epochs=100,n_batch=128):\n",
        "  half_batch=int(n_batch / 2)\n",
        "  for epoch in range(n_epochs):\n",
        "        #Generate real samples\n",
        "        x_real,y_real=generate_real_samples(half_batch)\n",
        "\n",
        "        #Train discriminator on real samples\n",
        "        d_loss_real,_=discriminator.train_on_batch(x_real,y_real)\n",
        "\n",
        "        #Generate fake samples\n",
        "        x_fake=generate_latent_points(latent_dim,half_batch)\n",
        "        y_fake=np.zeros((half_batch,1))\n",
        "\n",
        "        #Train discriminator on fake samples\n",
        "        d_loss_fake,_=discriminator.train_on_batch(generator.predict(x_fake),y_fake)\n",
        "\n",
        "        #Generate latent points as input for the generator\n",
        "        x_gan=generate_latent_points(latent_dim,n_batch)\n",
        "\n",
        "        #Create labels for generated samples(real ones)\n",
        "        y_gan=np.ones((n_batch,1))\n",
        "\n",
        "        #Train the generator via the discriminators error\n",
        "        g_loss=gan_model.train_on_batch(x_gan,y_gan)\n",
        "\n",
        "        #print the progress\n",
        "        if(epoch+1) %2==0:\n",
        "          print(f\"Epoch {epoch +1}, D Real loss:{d_loss_real},D Fake Loss:{d_loss_fake},G Loss:{g_loss}\")\n",
        "\n",
        "#Set parameters\n",
        "latent_dim=5\n",
        "\n",
        "#Define and compile the discriminator\n",
        "discriminator=define_discriminator()\n",
        "discriminator.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "\n",
        "#Define the generator\n",
        "generator=define_generator(latent_dim)\n",
        "\n",
        "#define the GAN model\n",
        "gan_model=define_gan(generator,discriminator)\n",
        "\n",
        "#Train the GAN\n",
        "train_gan(generator,discriminator,gan_model,latent_dim)\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}